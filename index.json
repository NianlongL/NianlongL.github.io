[{"authors":null,"categories":null,"content":"Nianlong Li is an Associate Professor at the Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences (ISCAS), and is supported by the Hundred Talents Plan of Chinese Academy of Sciences. Prior to joining ISCAS, Nianlong was a researcher at Lenovo Research, where he contributed to core interaction technologies and platforms for the company\u0026rsquo; s glasses-free 3D product line. He received his Ph.D. in Computer Science from the University of Chinese Academy of Sciences (UCAS) in 2022, under the supervision of Prof. Feng Tian and Prof. Teng Han.\nNianlong\u0026rsquo;s research focuses on Human-Computer Interaction (HCI), with a particular emphasis on spatial interaction paradigms for 3D interfaces and embodied intelligent agents. His research interests include: 1) Modeling human behaviors to uncover interaction mechanisms; 2) Optimizing devices and multimodal techniques to improve task efficiency; 3) Transferring user abilities to agents to advance human-machine collaboration.\n Download my CV.\r\r--\r\rI am looking for both long-term and short-term interns. If interested, drop me an email.\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://NianlongL.github.io/author/nianlong-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nianlong-li/","section":"authors","summary":"Nianlong Li is an Associate Professor at the Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences (ISCAS), and is supported by the Hundred Talents Plan of Chinese Academy of Sciences.","tags":null,"title":"Nianlong Li","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026quot;Courses\u0026quot;\rurl = \u0026quot;courses/\u0026quot;\rweight = 50\r Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026quot;Docs\u0026quot;\rurl = \u0026quot;docs/\u0026quot;\rweight = 50\r Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://NianlongL.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://NianlongL.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://NianlongL.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\r\r\rSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://NianlongL.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Jingwei Sun, Zhongyue Zhang, Mengyang Wang, **Nianlong Li**, Zhangwei Lu, Yan Xiang, Liuxin Zhang","Yu Zhang*","Qianying Wang*","Mingming Fan*"],"categories":[],"content":"","date":1745653719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745653719,"objectID":"8ea39c7328fb323011a384a2970e16e9","permalink":"https://NianlongL.github.io/publication/chi2025-chorusofpast/","publishdate":"2025-04-26T15:48:39+08:00","relpermalink":"/publication/chi2025-chorusofpast/","section":"publication","summary":"Reminiscence has been shown to provide benefits for older adults, but traditionally relies on personal photos as memory cues and interactions with real people who may not always be available. We present ReminiBuddy, a novel LLM-powered multi-agent conversational system, which allows older adults to engage with two distinct agents—one embodying an older identity and the other a younger identity—while using not only personal photos but also 3D models of generic nostalgic objects as memory cues. Our study, with older adult participants, found that the conversational approach both enjoyable and beneficial for reminiscence. While the younger agent was perceived as more emotionally engaging, the older one fostered greater resonance in content. Personal photos prompted autobiographical memories, whereas 3D generic nostalgic objects evoked shared memories of an era, contributing to a more multifaceted reminiscence experience. We further present design implications for better supporting older adults in reminiscing with LLM-powered conversational agents.","tags":[],"title":"(CHI '25) Chorus of the Past: Toward Designing a Multi-agent Conversational Reminiscence System with Digital Artifacts for Older Adults","type":"publication"},{"authors":["Tianren Luo, Tong Wu, Chaoyong Jiang, Xinran Duan, Jiafu Lv, **Nianlong Li**, Yachun Fan","Teng Han*","Feng Tian*"],"categories":[],"content":"","date":1745653719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745653719,"objectID":"6efadcd9f303477eebb3f08e21886f70","permalink":"https://NianlongL.github.io/publication/chi2025-remapvr/","publishdate":"2025-04-26T15:48:39+08:00","relpermalink":"/publication/chi2025-remapvr/","section":"publication","summary":"Remapping techniques in VR such as repositioning, redirection, and resizing have been extensively studied. Still, interaction designers rarely have the opportunity to use them due to high technical and knowledge barriers. In the paper, we extract common features of 24 existing remapping techniques and develop a high-fidelity immersive authoring tool, namely RemapVR, for rapidly building and experiencing prototypes of remapped space properties in VR that are unperceivable or acceptable to users. RemapVR provides designers with a series of functions for editing remappings and visualizing spatial property changes, mapping relationships between real and virtual worlds, sensory conflicts, etc. Designers can quickly build existing remappings via templates, and author new remappings by interactively recording spatial relations between input trajectory in real world and output trajectory in virtual world. User studies showed that the designs of RemapVR can effectively improve designers’ authoring experience and efficiency, and support designers to author remapping prototypes that meet scene requirements and provide good user experience.","tags":[],"title":"(CHI '25) RemapVR: An Immersive Authoring Tool for Rapid Prototyping of Remapped Interaction in VR","type":"publication"},{"authors":["**Nianlong Li**, Tong Wu, Zhenxuan He, Luyao Shen, Tianren Luo, Teng Han, BoYu Gao, Yu Zhang, Liuxin Zhang, Feng Tian","Qianying Wang*"],"categories":[],"content":"","date":1741765719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741765719,"objectID":"213ad38e7ad428600bef4a690ee0bd5e","permalink":"https://NianlongL.github.io/publication/vr2025-dualstick/","publishdate":"2025-03-12T15:48:39+08:00","relpermalink":"/publication/vr2025-dualstick/","section":"publication","summary":"This work presents Dual-Stick, a novel controller with two sticks connected at the end that innovates a Dual-Ray interaction paradigm to enrich raycasting input in Virtual Reality (VR). Dual-Stick leverages the inherent human dexterity in using everyday tools such as clamps and tweezers to adjust the relative angle between two sticks. This design supports Dual-Ray interactions that provide with a heuristics-based enhanced mechanism. It also offers more flexible manipulation by taking advantages of additional degrees of freedom provided by clamping angle. We conducted two studies to evaluate the effectiveness of Dual-Ray in target selection and manipulation tasks. The results indicated that Dual-Ray significantly improved efficiency in target selection compared to single-ray input but did not outperform the enhanced single-ray technique. In terms of manipulation, Dual-Ray effectively reduced completion time and mode switching compared to single-ray input.","tags":[],"title":"(IEEE VR '25) A Dual-Stick Controller for Enhancing Raycasting Interactions with Virtual Objects","type":"publication"},{"authors":["Chutian Jiang","Yanjun Chen","Mingming Fan","Liuping Wang","Luyao Shen","**Nianlong Li**","Wei Sun","Yu Zhang","Feng Tian","Teng Han*"],"categories":[],"content":"","date":1626238210,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626238210,"objectID":"e057ea36ab77b66c4293a71a2c398405","permalink":"https://NianlongL.github.io/publication/imwut2021-vrpain/","publishdate":"2021-07-14T12:50:10+08:00","relpermalink":"/publication/imwut2021-vrpain/","section":"publication","summary":"The imitation of pain sensation in Virtual Reality is considered valuable for safety education and training but has been seldom studied. This paper presents Douleur, a wearable haptic device that renders intensity-adjustable pain sensations with chemical stimulants. Different from mechanical, thermal, or electric stimulation, chemical-induced pain is more close to burning sensations and long-lasting. Douleur consists of a microfluidic platform that precisely emits capsaicin onto the skin and a microneedling component to help the stimulant penetrate the epidermis layer to activate the trigeminal nerve efficiently. Moreover, it embeds a Peltier module to apply the heating or cooling stimulus to the affected area to adjust the level of pain on the skin. To better understand how people would react to the chemical stimulant, we conducted a first study to quantify the enhancement of the sensation by changing the capsaicin concentration, skin temperature, and time and to determine suitable capsaicin concentration levels. In the second study, we demonstrated that Douleur could render a variety of pain sensations in corresponding virtual reality applications. In sum, Douleur is the first wearable prototype that leverages a combination of capsaicin and Peltier to induce rich pain sensations and opens up a wide range of applications for safety education and more.","tags":[],"title":"(IMWUT '21) Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality","type":"publication"},{"authors":["**Nianlong Li**","Zhengquan Zhang","Can Liu","Zengyao Yang*","Yinan Fu","Feng Tian","Teng Han","Mingming Fan"],"categories":[],"content":"","date":1610610519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610610519,"objectID":"cb4572cbe7ffe18806149cf6dca3e27e","permalink":"https://NianlongL.github.io/publication/chi2021-vmirror/","publishdate":"2021-01-14T15:48:39+08:00","relpermalink":"/publication/chi2021-vmirror/","section":"publication","summary":"Interacting with out of reach or occluded VR objects can be cumbersome. Although users can change their position and orientation, such as via teleporting, to help observe and select, doing so frequently may cause loss of spatial orientation or motion sickness. We present vMirror, an interactive widget leveraging reflection of mirrors to observe and select distant or occluded objects. We first designed interaction techniques for placing mirrors and interacting with objects through mirrors. We then conducted a formative study to explore a semi-automated mirror placement method with manual adjustments. Next, we conducted a target-selection experiment to measure the effect of the mirror's orientation on users' performance. Results showed that vMirror can be as efficient as direct target selection for most mirror orientations. We further compared vMirror with teleport technique in a virtual treasure hunt game and measured participants' task performance and subjective experiences. Finally, we discuss vMirorr user experience and present future directions.","tags":[],"title":"(CHI '21) vMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors","type":"publication"},{"authors":["**Nianlong Li**","Han-Jong Kim","Luyao Shen","Feng Tian","Teng Han*","Xing-Dong Yang","Tek-Jin Nam  Best Paper Honorable Mention Award  "],"categories":[],"content":"","date":1601105191,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601105191,"objectID":"ecd5d7daaf209b4dbdc69d3c6185492e","permalink":"https://NianlongL.github.io/publication/uist2020-haplinkage/","publishdate":"2020-09-26T15:26:31+08:00","relpermalink":"/publication/uist2020-haplinkage/","section":"publication","summary":"Haptic simulation of hand tools like wrenches, pliers, scissors and syringes are beneficial for finely detailed skill training in VR, but designing for numerous hand tools usually requires an expert-level knowledge of specific mechanism and protocol. This paper presents HapLinkage, a prototyping framework based on linkage mechanism, that provides typical motion templates and haptic renderers to facilitate proxy design of virtual hand tools. The mechanical structures can be easily modified, for example, to scale the size, or to change the range of motion by selectively changing linkage lengths. Resistant, stop, release, and restoration force feedback are generated by an actuating module as part of the structure. Additional vibration feedback can be generated with a linear actuator. HapLinkage enables easy and quick prototypting of hand tools for diverse VR scenarios, that embody both of their kinetic and haptic properties. Based on interviews with expert designers, it was confirmed that HapLinkage is expressive in designing haptic proxy of hand tools to enhance VR experiences. It also identified potentials and future development of the framework.","tags":[],"title":"(UIST '20) HapLinkage: Prototyping Haptic Proxies for Virtual Hand Tools Using Linkage Mechanism","type":"publication"},{"authors":["**Nianlong Li**","Teng Han*","Feng Tian","Jin Huang","Minghui Sun","Pourang Irani","Jason Alexander  Best Paper Honorable Mention Award  "],"categories":[],"content":"","date":1601020160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601020160,"objectID":"3c199d3679a12473c20c76110f59197a","permalink":"https://NianlongL.github.io/publication/chi2020-vrpen/","publishdate":"2020-09-25T15:49:20+08:00","relpermalink":"/publication/chi2020-vrpen/","section":"publication","summary":"The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.","tags":[],"title":"(CHI '20) Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen","type":"publication"},{"authors":[],"categories":[],"content":"Paper accepted to UIST 2020 and won an Honorable Mention Award. Congrats to co-authors from KAIST and Dartmouth.\n","date":1598703513,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598703513,"objectID":"112f2b2a4030132561f0a3c271ee7269","permalink":"https://NianlongL.github.io/post/20200829/","publishdate":"2020-08-29T20:18:33+08:00","relpermalink":"/post/20200829/","section":"post","summary":"Paper accepted to UIST 2020 and won an Honorable Mention Award. Congrats to co-authors from KAIST and Dartmouth.","tags":[],"title":"","type":"post"},{"authors":["Jin Huang","Nianlong Li","Feng Tian","Hongan Wang"],"categories":[],"content":"","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"154e31509103a6b9a467a13be286f9cb","permalink":"https://NianlongL.github.io/post/20200306/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/post/20200306/","section":"post","summary":"Filed: 2018-06-01. Patent No. CN 2018105586224","tags":[],"title":"A Moving Target Selection Technology based on Correction of User Performance Model. ","type":"post"},{"authors":["Jin Huang","Feng Tian*","**Nianlong Li**","Xiangmin Fan"],"categories":[],"content":"","date":1571556659,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571556659,"objectID":"3b07ec7d2484b9714f9f86ecbf0d6bb5","permalink":"https://NianlongL.github.io/publication/uist2019-2dselection/","publishdate":"2019-10-23T15:30:59+08:00","relpermalink":"/publication/uist2019-2dselection/","section":"publication","summary":"Understanding the selection uncertainty of moving targets is a fundamental research problem in HCI. However, the only few works in this domain mainly focus on selecting 1D moving targets with certain input devices, where the model generalizability has not been extensively investigated. In this paper, we propose a 2D Ternary-Gaussian model to describe the selection uncertainty manifested in endpoint distribution for moving target selection. We explore and compare two candidate methods to generalize the problem space from 1D to 2D tasks, and evaluate their performances with three input modalities including mouse, stylus, and finger touch. By applying the proposed model in assisting target selection, we achieved up to 4% improvement in pointing speed and 41% in pointing accuracy compared with two state-of-the-art selection technologies. In addition, when we tested our model to predict pointing errors in a realistic user interface, we observed high fit of 0.94 R2.","tags":[],"title":"(UIST '19) Modeling the Uncertainty in 2D Moving Target Selection","type":"publication"},{"authors":["**Nianlong Li**","Feng Tian*","Xiangmin Fan","Yicheng Zhu","Hongan Wang","Guozhong Dai"],"categories":[],"content":"","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"3cd6c4e5220606d489fd75fc9d6e7a37","permalink":"https://NianlongL.github.io/publication/ccftpci2019-spoon/","publishdate":"2019-05-27T16:56:27+08:00","relpermalink":"/publication/ccftpci2019-spoon/","section":"publication","summary":"Daily monitoring of Parkinson's disease is important since clinical assessments can only provide a brief and limited view of a patient's condition. However, traditional approaches rely heavily on patients' self-reports or diaries, which are subjective and often lack of necessary details. In this work, we instrument a handle that can be attached to cutlery with inertial sensors to collect motion data unobtrusively. By analyzing the data of patients and normal people collected in the clinic, we demonstrated that our machine learning based model can not only distinguish between patients and normal people, but also identify the disease levels in a fine-grained manner. To further understand how the self-tracking data is used in clinic, we conducted a semi-structured interview with several clinicians. Through the interpretation from the perspective of both physicians and patients, we found that our handle can help the physicians better understand disease progression and promote patients' engagement in tackling the disease.","tags":[],"title":"(CCFTPCI '19) Monitoring motor symptoms in Parkinson's disease via instrumenting daily artifacts with inertia sensors","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic \rAcademic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click \rPDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? \rAsk\n\rDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://NianlongL.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["**Nianlong Li**","Feng Tian*","Jin Huang","Xiangmin Fan","Hongan Wang"],"categories":[],"content":"","date":1524732575,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524732575,"objectID":"cfb17897a30ee188ede8386d47b2bc78","permalink":"https://NianlongL.github.io/publication/chilbw2018-2dbayespointer/","publishdate":"2020-10-06T16:49:35+08:00","relpermalink":"/publication/chilbw2018-2dbayespointer/","section":"publication","summary":"Interactive systems with dynamic content are becoming ubiquitous nowadays. However, it is challenging to select small and fast-moving targets in such environment. We present 2D-BayesPointer, a novel interaction technique to assist moving target selection in 2D space. Compared with previous techniques, our method provides implicit support without modifying the original interface design. Moreover, the algorithmic parameters are determined by probabilistic modeling of human performance in moving target selection tasks. The preliminary results from a pilot study have shown that this technique can significantly improve both selection speed and accuracy.","tags":[],"title":"(CHI LBW '18) 2D-BayesPointer: An Implicit Moving Target Selection Technique Enabled by Human Performance Modeling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://NianlongL.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://NianlongL.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]